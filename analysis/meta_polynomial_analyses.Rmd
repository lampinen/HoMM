---
title: "Meta polynomial analyses"
output: html_notebook
---

```{r}
library(tidyverse)
library(stargazer)
```

```{r}
num_runs = 5
results_dir = "../results/newest_results/"

result_subdirs = c("basic_ADAM_nobinary_slower", "untrained_baseline", "nometa_fastrun", "slower_integration")
language_result_subdirs = c("language_meta_only", "language_conditioned_meta_only")
language_only_subdirs = c() # missing other losses


result_subdirs = c(result_subdirs, language_result_subdirs)

all_result_subdirs = c(result_subdirs, language_only_subdirs)
language_result_subdirs = c(language_result_subdirs, language_only_subdirs)

name_run_type = function(run_type) { # names for plotting
  case_when(run_type == "basic_ADAM_nobinary_slower" ~ "EML",
            run_type == "language_meta_only" ~ "Language",
            run_type == "language_conditioned_meta_only" ~ "Language (non-hyper)",
            # run_type == "language_only" ~ "Language + Hyper (no meta)",
            # run_type == "conditioned_language_baseline" ~ "Language + FF",
            run_type == "slower_integration" ~ "EML",
            run_type == "basic_separate_meta" ~ "EML",
            run_type == "nometa_fastrun" ~ "EML (no meta tasks)",
            run_type == "untrained_baseline" ~ "Untrained EML network",
            run_type == "hyper_continual_frozen" ~ "Tuning new task embeddings",
            run_type == "hyper_continual_frozen_untrained_baseline" ~ "... in an untrained network",
            run_type == "hyper_continual_frozen_randomly_initialized_baseline" ~ "... from random embeddings",
            T ~ run_type)
}
```

# utils and setup
```{r}
read_config = function(config_file) { 
  config = read_delim(config_file, delim="\n") %>%
    separate(`key, value`, c("key", "value"), sep=",", extra="merge") %>%
    spread(key, value) %>%
    mutate_at(c("base_task_names", "new_task_names", "base_meta_tasks", "base_meta_binary_funcs", "base_meta_mappings", "new_meta_mappings"), function(x) {
      x = gsub("\\\"|[][]| |\'", "", x)
      return(str_split(x, ","))
    } )
}
```

```{r}
load_d = function(results_dir, result_subdirs, num_runs, file_type) {
  d = data.frame()
  for (run_i in 0:(num_runs-1)) {
    for (result_subdir in result_subdirs) {
      filename = sprintf("%s%s/run%i_%s.csv", results_dir, result_subdir, run_i, file_type)
      if (!file.exists(filename)) {
        next
      }
      if (file_type == "config") {
        this_d = read_config(filename)
      } else {
        this_d = read.csv(filename, check.names=F, header=T) 
        this_d = this_d[, !duplicated(colnames(this_d))] # drop functions which are equivalent within rounding 
      }
      this_d = this_d %>%
        mutate(run = run_i,
               run_type = result_subdir)
      d = d %>%
        bind_rows(this_d)
    }
    
  }
  d = d %>%
    mutate(named_run_type = name_run_type(run_type))
  return(d)
}
```

plot themes
```{r}
theme_set(theme_bw() + 
            theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                  legend.margin = margin(0, 0, 0, 0), 
                  legend.box.margin = margin(0, 0, 0, 0)))
```

```{r}
name_var = function(var) { # var names for plotting
  case_when(var %in% c("is_new", "is_new_polynomial") ~ "Held out?",
            var %in% c("run_type", "named_run_type") ~ "Run type",
            var == "loss" ~ "Average loss",
            var == "result_type" ~ "Hold-out type",
            T ~ var)
  }

summary_plot = function(data,  x_var, y_var, color_var, optimal_baseline_data=NULL, other_baseline_data=NULL, palette="Set2", continuous_x=F) {
  if(x_var == color_var | continuous_x) {
    pos_func=position_identity()
  } else {
    pos_func = position_dodge(width=0.5)
  }
  
  p = ggplot(data %>% 
               filter(run_type != "untrained_baseline"),
             aes_string(x=x_var, y=y_var, color=color_var)) +
    geom_hline(yintercept=0, alpha=0.33)
  
  if ("untrained_baseline" %in% unique(data$run_type)) {
    p = p + 
      geom_hline(data=data %>% 
                   filter(run_type == "untrained_baseline") %>% 
#                   group_by_at(vars(one_of(x_var, color_var))) %>%
                   summarize_at(y_var, mean, na.rm=T),
                 aes_string(yintercept=y_var),
                 linetype=2,
                 alpha=0.5) 
  }
  
  p = p +
    geom_point(stat="summary", fun.y=mean, size=2,
               position=pos_func) +
    geom_errorbar(stat="summary", 
                  fun.ymin=function(x) {mean(x) - 1.96*sd(x)/sqrt(length(x))},
                  fun.ymax=function(x) {mean(x) + 1.96*sd(x)/sqrt(length(x))},
                  width=0.4,
                  position=pos_func) +
    scale_color_manual(values=c("#555093", "#f38f22", "#88e1e1", "#008822"))
  
  p = p + 
    #scale_color_brewer(palette=palette) +
    labs(x=name_var(x_var), y=name_var(y_var)) +
    guides(color=guide_legend(title=NULL))
  
  if (continuous_x) {
    p = p +
      geom_line(stat="summary", fun.y=mean, size=2, 
                position=pos_func)
  }
    
  if (!is.null(optimal_baseline_data)) {
    # TODO (maybe)
  }
  
  if (!is.null(other_baseline_data)) {
    # TODO (maybe)
  }
  
  return(p)
}
```

```{r}
config_d = load_d(results_dir, all_result_subdirs, num_runs, "config")
```

```{r}
check_if_base_task = function(run_num, run_type, task_name) {
  base_task_names = config_d %>%
    filter(run == run_num, run_type == run_type) %>%
    pull(base_task_names) %>%
    unlist()
  return(task_name %in% base_task_names)
}
```

```{r}
check_if_new_task = function(run_num, run_type, task_name) {
  new_task_names = config_d %>%
    filter(run == run_num, run_type == run_type) %>%
    pull(new_task_names) %>%
    unlist()
  return(task_name %in% new_task_names)
}
```

N.B. that because the new and old tasks are sampled independently, there are occasionally collisions, especially with constant polynomials (not that many options, really). It seems to be rare enough to not materially affect the conclusions, however.

# meta-learning tests

```{r}
base_d = load_d(results_dir, result_subdirs, num_runs, "new_losses") %>% 
  filter(epoch == 0 | run_type == "slower_integration") %>%
  select(epoch, run, named_run_type, run_type,
         matches("\\d\\.\\d\\d"), -matches("add|permute|mult|square")) %>% # all base tasks have at least one coefficient
  gather(polynomial, loss, -epoch, -contains("run"))
```

```{r}
base_d = base_d %>%
   rowwise() %>%
   mutate(is_new = check_if_new_task(run, run_type, polynomial),
          is_base = check_if_base_task(run, run_type, polynomial)) %>%

  ungroup() %>%
  filter(is_new | is_base) %>% # don't eval implied (although results are similar) because distribution is skewed
  mutate(learned="After training",
         is_new = factor(is_new, labels=c("Trained", "Held out")))
```

```{r}
summary_plot(base_d %>% 
               filter(epoch == 0),
             "named_run_type", "loss", "is_new") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
summary_plot(base_d %>%
               filter(epoch == 0,
                      run_type %in% c("untrained_baseline", "basic_ADAM_nobinary_slower")),
             "is_new", "loss", "is_new") 
ggsave("../../meta_RL/writing/figures/poly/basic_results.png", width=4.2, height=3)
```

```{r}
base_d %>%
  filter(epoch == 0,
         run_type %in% c("untrained_baseline", "basic_ADAM_nobinary_slower")) %>%
  group_by(named_run_type, is_new) %>%
  do(data.frame(mean_cl_boot(.$loss))) %>%
  rename(mean_loss=y, boot_CI_low=ymin, boot_CI_high=ymax) %>%
  ungroup() %>%
  mutate(is_new=as.character(is_new)) %>%
  mutate_at(vars(mean_loss, contains("CI")), function(x) {round(x, digits=3)}) %>%
  stargazer(summary = F, rownames = F, digits=2)
```
```{r}
summary_plot(base_d %>%
               filter(run_type %in% c("slower_integration")),
             "epoch", "loss", "is_new",
             continuous_x = T) +
  labs(x="Epoch after introducing new tasks")
ggsave("../../meta_RL/writing/figures/poly/basic_results_over_time.png", width=4.2, height=3)
```

# meta task learning


```{r}
meta_d = load_d(results_dir, result_subdirs, num_runs, "new_meta_true_losses") %>% 
  filter(epoch == 0) 
```

```{r}
check_if_new_meta_mapping = function(run_num, run_type, meta_task) {
  new_meta_mapping_names = config_d %>%
    filter(run == run_num, run_type == run_type) %>%
    pull(new_meta_mappings) %>%
    unlist()
  return(meta_task %in% new_meta_mapping_names)
}
```

```{r}
process_meta_d = function(meta_d) {
  meta_d = meta_d %>%
    gather(meta_task, loss, -epoch, -contains("run"), na.rm=T) %>% 
    separate(meta_task, c("meta_task", "source", "target"), sep=":|->") %>%
    rowwise() %>%
    mutate(is_new_argument = check_if_new_task(run, run_type, source),
           is_new_target = check_if_new_task(run, run_type, target),
           is_new_meta_mapping = check_if_new_meta_mapping(run, run_type, meta_task)) %>%
    ungroup() %>%
    mutate(result_type=case_when(!is_new_argument & !is_new_meta_mapping ~ "Trained mapping, on trained task",
                          is_new_argument & !is_new_meta_mapping ~ "Trained mapping, on held-out task",
                          !is_new_argument & is_new_meta_mapping ~ "Held-out mapping, on trained task",
                          is_new_argument & is_new_meta_mapping ~ "Held-out mapping, on held-out task"),
           result_type = factor(result_type, levels = c('Trained mapping, on trained task', 'Trained mapping, on held-out task', 'Held-out mapping, on trained task', 'Held-out mapping, on held-out task')))
  return(meta_d)
}
```

```{r}
meta_d = process_meta_d(meta_d) %>%
  mutate(learned="After training")
```

```{r}
summary_plot(meta_d %>%
               filter(run_type %in% c("language_meta_only", "language_conditioned_meta_only")),
             "named_run_type", "loss", "result_type") +
  ylim(0, NA) +
  labs(y="Performance after meta-mapping") +
  geom_hline(yintercept=0) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("../../meta_RL/writing/figures/poly/meta_results_conditioned_vs_hyper.png", width=4.2, height=3)
```

```{r}
summary_plot(meta_d %>%
               filter(run_type %in% c("untrained_baseline", "basic_ADAM_nobinary_slower"),
                      !grepl("binary", meta_task)),
             "result_type", "loss", "result_type") +
  ylim(0, NA) +
  labs(y="Loss on target task") +
  scale_y_continuous(breaks=seq(0, 12, 3)) +
  scale_x_discrete(labels = function(x) {str_wrap(x, width=18)})+
  guides(color=F) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("../../meta_RL/writing/figures/poly/meta_results.png", width=4.2, height=3)
```

```{r}
summary_plot(meta_d %>%
               filter(run_type != "basic",
                      grepl("binary", meta_task)),
             "named_run_type", "loss", "is_new") +
  ylim(0, NA) +
  facet_wrap(~meta_task)
```

# language tests

```{r}
# original_lang_d = load_d(results_dir, language_result_subdirs, num_runs, "language_losses") %>% 
#   filter(epoch == 0) %>%
#   select(epoch, run, named_run_type, run_type,
#          matches("\\d\\.\\d\\d")) %>% # all base tasks have at least one coefficient
#   gather(polynomial, loss, -epoch, -contains("run")) %>%
#   mutate(is_new=F, learned="Before training")
```

```{r}
lang_d = load_d(results_dir, language_result_subdirs, num_runs, "new_language_losses") %>% 
  filter(epoch == 0) %>%
  select(epoch, run, named_run_type, run_type,
         matches("\\d\\.\\d\\d"), -matches("add|permute|mult|square")) %>% # all base tasks have at least one coefficient
  gather(polynomial, loss, -epoch, -contains("run"))
```

```{r}
lang_d = lang_d %>%
  rowwise() %>%
  mutate(is_new = check_if_new_task(run, run_type, polynomial)) %>%
  ungroup() %>%
  mutate(learned="After training")
```

```{r}
#lang_d = bind_rows(original_lang_d, lang_d)
```

```{r}
summary_plot(lang_d %>%
               filter(run_type != "language"),
             "named_run_type", "loss", "is_new") 
  
```

```{r}
summary_plot(lang_d %>%
               filter(is_new,
                      grepl("X", polynomial)), # nonconstant
             "learned", "loss", "is_new") +
  facet_grid(named_run_type ~ .) 
  
```

## language meta tests


```{r}
lang_meta_d = load_d(results_dir, result_subdirs, num_runs, "newlang_meta_true_losses") %>% 
  filter(epoch == 0) 
```

```{r}
lang_meta_d = process_meta_d(lang_meta_d) %>%
  mutate(learned="After training")
```

add baseline data
```{r}
lang_meta_d = bind_rows(lang_meta_d,
                        meta_d %>%
                          filter(run_type == "untrained_baseline",
                                 !grepl("binary", meta_task)))
```

```{r}
summary_plot(lang_meta_d %>%
               filter(run_type %in% c("language_meta_only", "untrained_baseline")),
             "result_type", "loss", "result_type") +
  ylim(0, NA) +
  labs(y="Loss on target task") +
  scale_y_continuous(breaks=seq(0, 12, 3)) +
  scale_x_discrete(labels = function(x) {str_wrap(x, width=18)}) +
  guides(color=F) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("../../meta_RL/writing/figures/poly/language_meta_results.png", width=4.2, height=3)
```

```{r}
summary_plot(lang_meta_d,
             "named_run_type", "loss", "result_type") +
  ylim(0, NA) +
  labs(y="Loss on target task") +
  scale_y_continuous(breaks=seq(0, 21, 3)) +
  scale_x_discrete(labels = function(x) {str_wrap(x, width=18)}) +
  guides(color=F) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggsave("../../meta_RL/writing/figures/poly/language_meta_conditioned_vs_hyper.png", width=6, height=3)
```

## comparing architectures

```{r}
num_runs_arch_comp = 5
results_dir_arch_comp = "../results/new_results/"

language_result_subdirs_arch_comp = c("untrained_baseline")
language_only_subdirs_arch_comp = c("language_only", "conditioned_language_baseline") # missing other losses

language_result_subdirs_arch_comp = c(language_result_subdirs_arch_comp, language_only_subdirs_arch_comp)
```

```{r}
lang_arch_comp_d = load_d(results_dir_arch_comp, language_result_subdirs_arch_comp, num_runs_arch_comp, "new_language_losses") %>% 
  select(epoch, run, named_run_type, run_type,
         matches("\\d\\.\\d\\d"), -matches("add|permute|mult|square")) %>% # all base tasks have at least one coefficient
  gather(polynomial, loss, -epoch, -contains("run"))
```

```{r}
arch_comp_config_d = load_d(results_dir_arch_comp, language_result_subdirs_arch_comp, num_runs_arch_comp, "config")

check_if_new_task_arch_comp = function(run_num, run_type, task_name) {
  new_task_names = arch_comp_config_d %>%
    filter(run == run_num, run_type == run_type) %>%
    pull(new_task_names) %>%
    unlist()
  return(task_name %in% new_task_names)
}
```

```{r}
lang_arch_comp_d = lang_arch_comp_d %>%
  rowwise() %>%
  mutate(is_new = check_if_new_task_arch_comp(run, run_type, polynomial)) %>%
  ungroup() %>%
  mutate(learned="After training")
```

```{r}
summary_plot(lang_arch_comp_d %>%
               filter(epoch == 0, run <= 3), # nonconstant
             "learned", "loss", "is_new") +
  facet_wrap(. ~ named_run_type) 
```

```{r}
summary_plot(lang_arch_comp_d %>%
               filter(run <= 3,
                      is_new), # nonconstant
             "epoch", "loss", "named_run_type",
             continuous_x=T)  
```

# continual learning 
Note: the code for running these experiments is in the continual_learning branch, but the analysis code is here to facilitate generating unified figures for the paper.

```{r}
continual_results_dir = "../results/continual_learning/"
continual_result_subdirs = c("hyper", "hyper_emb_only_even_faster",  "hyper_emb_only_even_faster_randomly_initialized_baseline",  "hyper_emb_only_even_faster_untrained_baseline") 
```

```{r}
continual_d = load_d(continual_results_dir, continual_result_subdirs, num_runs, "new_losses") %>% 
  select(epoch, run, named_run_type, run_type,
         matches("\\d\\.\\d\\d"), -matches("add|permute|mult|square")) %>% # all base tasks have at least one coefficient
  gather(polynomial, loss, -epoch, -contains("run")) %>% 
  mutate(run_type = case_when(run_type == "hyper" ~ "hyper_continual", 
                              run_type == "conditioned" ~ "conditioned_continual",
                              run_type == "hyper_emb_only_even_faster" ~ "hyper_continual_frozen",
                              run_type == "hyper_emb_only_even_faster_untrained_baseline" ~ "hyper_continual_frozen_untrained_baseline",
                              run_type == "hyper_emb_only_even_faster_randomly_initialized_baseline" ~ "hyper_continual_frozen_randomly_initialized_baseline",
                              run_type == "conditioned_emb_only_even_faster" ~ "conditioned_continual_frozen",
                              run_type == "conditioned_emb_only_even_faster_untrained_baseline" ~ "conditioned_continual_frozen_untrained_baseline",
                              T ~ run_type),
         named_run_type = name_run_type(run_type))
```


```{r}
continual_config_d = load_d(continual_results_dir, continual_result_subdirs, num_runs, "config") %>% 
  mutate(run_type = case_when(run_type == "hyper" ~ "hyper_continual", 
                              run_type == "conditioned" ~ "conditioned_continual",
                              run_type == "hyper_emb_only_even_faster" ~ "hyper_continual_frozen",
                              run_type == "hyper_emb_only_even_faster_untrained_baseline" ~ "hyper_continual_frozen_untrained_baseline",
                              run_type == "hyper_emb_only_even_faster_randomly_initialized_baseline" ~ "hyper_continual_frozen_randomly_initialized_baseline",
                              run_type == "conditioned_emb_only_even_faster" ~ "conditioned_continual_frozen",
                              run_type == "conditioned_emb_only_even_faster_untrained_baseline" ~ "conditioned_continual_frozen_untrained_baseline",
                              T ~ run_type),
         named_run_type = name_run_type(run_type))

```

```{r}
config_d = bind_rows(config_d, continual_config_d)
```

```{r}
continual_d = continual_d %>%
  rowwise() %>%
  mutate(is_new = check_if_new_task(run, run_type, polynomial)) %>%
  ungroup() %>%
  mutate(learned="After training",
         is_new = factor(is_new, labels=c("Prior tasks", "New tasks")),
         named_run_type = factor(named_run_type, levels=c("Tuning new task embeddings", "... from random embeddings", "... in an untrained network")))
```
  
```{r}
summary_plot(continual_d %>%
               filter(!grepl("conditioned", named_run_type),
                      run_type != "hyper_continual",
                      epoch <= 1000,
                      run_type != "hyper_continual_frozen_randomly_initialized_baseline" | epoch > 0), # the first eval step of random init is actually with guess embeddings
             "epoch", "loss", "is_new",
             continuous_x = T) +
  labs(x = "Epoch", y="Loss") +
  facet_wrap(~ named_run_type)
ggsave("../../meta_RL/writing/figures/continual/continual_learning.png", width=8, height=3)
```

```{r}
summary_plot(continual_d %>%
               filter(run_type %in% c("hyper_continual", "hyper_continual_frozen")), 
             "epoch", "loss", "is_new",
             continuous_x = T) +
  labs(x = "Epoch", y="Loss") +
  facet_wrap(~ named_run_type)
ggsave("../../meta_RL/writing/figures/continual/continual_learning_vs_full_integration_training.png", width=8, height=3)
```

```{r}
summary_plot(continual_d %>%
               filter(!grepl("conditioned", named_run_type),
                      run_type != "hyper",
                      run_type != "hyper_continual_frozen_randomly_initialized_baseline" | epoch > 0,
                      is_new == "New tasks"), # the first eval step of random init is actually with guess embeddings
             "epoch", "loss", "named_run_type",
             continuous_x = T) +
  labs(x = "Epoch", y="Loss on new tasks")
ggsave("../../meta_RL/writing/figures/continual/continual_learning_direct_comparison.png", width=6, height=3)
```

```{r}
library(lme4)
```

```{r}
lmer(loss ~ named_run_type + (1|run),
   data=continual_d %>% 
     group_by(run_type, named_run_type) %>%
     filter(epoch == 300) %>%
     ungroup()) %>%
  summary()
```
