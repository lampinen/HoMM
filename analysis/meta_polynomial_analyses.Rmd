---
title: "Meta polynomial analyses"
output: html_notebook
---

```{r}
library(tidyverse)
```

```{r}
# num_runs = 5
# results_dir = "../results/newer_results_no_binary/"
# 
# result_subdirs = c("basic_smaller")
# language_result_subdirs = c("language", "untrained_baseline")
# language_only_subdirs = c("language_only", "conditioned_language_baseline") # missing other losses

num_runs = 5
results_dir = "../results/newer_results_no_binary/"

result_subdirs = c("basic", "basic_md080", "basic_md085")
language_result_subdirs = c("language", "untrained_baseline")
language_only_subdirs = c() # missing other losses

result_subdirs = c(result_subdirs, language_result_subdirs)

all_result_subdirs = c(result_subdirs, language_only_subdirs)
language_result_subdirs = c(language_result_subdirs, language_only_subdirs)

name_run_type = function(run_type) { # names for plotting
  case_when(run_type == "basic_smaller" ~ "Basic (100 tasks)",
            run_type == "basic" ~ "Basic (no language)",
            run_type == "language" ~ "Language (full)",
            run_type == "language_only" ~ "Language + Hyper (no meta)",
            run_type == "conditioned_language_baseline" ~ "Language + FF",
            run_type == "untrained_baseline" ~ "Untrained",
            run_type == "hyper_continual_frozen" ~ "Tuning new task embeddings",
            run_type == "hyper_continual_frozen_untrained_baseline" ~ "... in an untrained network",
            run_type == "hyper_continual_frozen_randomly_initialized_baseline" ~ "... from random embeddings",
            T ~ run_type)
}
```

# utils and setup
```{r}
read_config = function(config_file) { 
  config = read_delim(config_file, delim="\n") %>%
    separate(`key, value`, c("key", "value"), sep=",", extra="merge") %>%
    spread(key, value) %>%
    mutate_at(c("base_task_names", "new_task_names", "base_meta_tasks", "base_meta_binary_funcs", "base_meta_mappings", "new_meta_mappings"), function(x) {
      x = gsub("\\\"|[][]| |\'", "", x)
      return(str_split(x, ","))
    } )
}
```

```{r}
load_d = function(results_dir, result_subdirs, num_runs, file_type) {
  d = data.frame()
  for (run_i in 0:(num_runs-1)) {
    for (result_subdir in result_subdirs) {
      filename = sprintf("%s%s/run%i_%s.csv", results_dir, result_subdir, run_i, file_type)
      if (!file.exists(filename)) {
        next
      }
      if (file_type == "config") {
        this_d = read_config(filename)
      } else {
        this_d = read.csv(filename, check.names=F, header=T) 
        this_d = this_d[, !duplicated(colnames(this_d))] # drop functions which are equivalent within rounding 
      }
      this_d = this_d %>%
        mutate(run = run_i,
               run_type = result_subdir)
      d = d %>%
        bind_rows(this_d)
    }
    
  }
  d = d %>%
    mutate(named_run_type = name_run_type(run_type))
  return(d)
}
```

plot themes
```{r}
theme_set(theme_bw() + 
            theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
                  legend.margin = margin(0, 0, 0, 0), 
                  legend.box.margin = margin(0, 0, 0, 0)))
```

```{r}
name_var = function(var) { # var names for plotting
  case_when(var %in% c("is_new", "is_new_polynomial") ~ "Held out?",
            var %in% c("run_type", "named_run_type") ~ "Run type",
            T ~ var)
  }

summary_plot = function(data,  x_var, y_var, color_var, optimal_baseline_data=NULL, other_baseline_data=NULL, palette="Set2", continuous_x=F) {
  if(x_var == color_var) {
    pos_func=NULL
  } else {
    pos_func = position_dodge(width=0.5)
  }
  p = ggplot(data, aes_string(x=x_var, y=y_var, color=color_var)) +
    geom_point(stat="summary", fun.y=mean, size=2,
               position=pos_func) +
    geom_errorbar(stat="summary", 
                  fun.ymin=function(x) {mean(x) - 1.96*sd(x)/sqrt(length(x))},
                  fun.ymax=function(x) {mean(x) + 1.96*sd(x)/sqrt(length(x))},
                  width=0.4,
                  position=pos_func) +
    scale_color_manual(values=c("#555093", "#f38f22", "#88e1e1"))
  
  if (continuous_x) {
    p = p + 
      geom_line(stat="summary", fun.y=mean, size=1.5) 
  }
  
  p = p + 
    #scale_color_brewer(palette=palette) +
    labs(x=name_var(x_var), y=name_var(y_var)) +
    guides(color=guide_legend(title=NULL))
    
  if (!is.null(optimal_baseline_data)) {
    # TODO (maybe)
  }
  
  if (!is.null(other_baseline_data)) {
    # TODO (maybe)
  }
  
  return(p)
}
```

```{r}
config_d = load_d(results_dir, all_result_subdirs, num_runs, "config")
```

```{r}
check_if_new_task = function(run_num, run_type, task_name) {
  new_task_names = config_d %>%
    filter(run == run_num, run_type == run_type) %>%
    pull(new_task_names) %>%
    unlist()
  return(task_name %in% new_task_names)
}
```

N.B. that because the new and old tasks are sampled independently, there are occasionally collisions, especially with constant polynomials (not that many options, really). It seems to be rare enough to not materially affect the conclusions, however.

# meta-learning tests

```{r}
# original_base_d = load_d(results_dir, result_subdirs, num_runs, "losses") %>% 
#   filter(epoch == 0) %>%
#   select(epoch, run, named_run_type, run_type,
#          matches("\\d\\.\\d\\d")) %>% # all base tasks have at least one coefficient
#   gather(polynomial, loss, -epoch, -contains("run")) %>%
#   mutate(is_new=F, learned="Before training")
```

```{r}
base_d = load_d(results_dir, result_subdirs, num_runs, "new_losses") %>% 
  filter(epoch == 0) %>%
  select(epoch, run, named_run_type, run_type,
         matches("\\d\\.\\d\\d"), -matches("add|permute|mult|square")) %>% # all base tasks have at least one coefficient
  gather(polynomial, loss, -epoch, -contains("run"))
```

```{r}
base_d = base_d %>%
  rowwise() %>%
  mutate(is_new = check_if_new_task(run, run_type, polynomial)) %>%
  ungroup() %>%
  mutate(learned="After training")
```

```{r}
#base_d = bind_rows(original_base_d, base_d)
```

```{r}
summary_plot(base_d,
             "named_run_type", "loss", "is_new") +
  geom_hline(yintercept=0)
  
```


# meta task learning

```{r}
# original_meta_d = load_d(results_dir, result_subdirs, num_runs, "meta_true_losses") %>% 
#   filter(epoch == 0) 
```

```{r}
meta_d = load_d(results_dir, result_subdirs, num_runs, "new_meta_true_losses") %>% 
  filter(epoch == 0) 
```

```{r}
process_meta_d = function(meta_d) {
  meta_d = meta_d %>%
    gather(meta_task, loss, -epoch, -contains("run"), na.rm=T) %>% 
    separate(meta_task, c("meta_task", "source", "target"), sep=":|->") %>%
    rowwise() %>%
    mutate(source_is_new = check_if_new_task(run, run_type, source),
           target_is_new = check_if_new_task(run, run_type, target)) %>%
    ungroup() %>%
    mutate(is_new = source_is_new | target_is_new)
  return(meta_d)
}
```

```{r}
meta_d = process_meta_d(meta_d) %>%
  mutate(learned="After training")
```

```{r}
# original_meta_d = process_meta_d(original_meta_d) %>%
#   mutate(learned="Before training")
```

```{r}
# meta_d = bind_rows(original_meta_d, meta_d)
```

```{r}
summary_plot(meta_d %>%
               filter(run_type == "untrained_baseline"),
             "named_run_type", "loss", "is_new") +
  ylim(0, NA)
```

```{r}
summary_plot(meta_d %>%
               filter(grepl("binary", meta_task)),
             "named_run_type", "loss", "is_new") +
  ylim(0, NA) +
  facet_wrap(~meta_task)
```

# language tests

```{r}
# original_lang_d = load_d(results_dir, language_result_subdirs, num_runs, "language_losses") %>% 
#   filter(epoch == 0) %>%
#   select(epoch, run, named_run_type, run_type,
#          matches("\\d\\.\\d\\d")) %>% # all base tasks have at least one coefficient
#   gather(polynomial, loss, -epoch, -contains("run")) %>%
#   mutate(is_new=F, learned="Before training")
```

```{r}
lang_d = load_d(results_dir, language_result_subdirs, num_runs, "new_language_losses") %>% 
  filter(epoch == 0) %>%
  select(epoch, run, named_run_type, run_type,
         matches("\\d\\.\\d\\d"), -matches("add|permute|mult|square")) %>% # all base tasks have at least one coefficient
  gather(polynomial, loss, -epoch, -contains("run"))
```

```{r}
lang_d = lang_d %>%
  rowwise() %>%
  mutate(is_new = check_if_new_task(run, run_type, polynomial)) %>%
  ungroup() %>%
  mutate(learned="After training")
```

```{r}
#lang_d = bind_rows(original_lang_d, lang_d)
```

```{r}
summary_plot(lang_d,
             "named_run_type", "loss", "is_new") 
  
```

```{r}
summary_plot(lang_d %>%
               filter(is_new,
                      grepl("X", polynomial)), # nonconstant
             "learned", "loss", "is_new") +
  facet_grid(named_run_type ~ polynomial) 
  
```

```{r}
summary_plot(lang_d %>%
               filter(grepl("X", polynomial)), # nonconstant
             "learned", "loss", "is_new") +
  facet_wrap(. ~ named_run_type) 
  
```

```{r}
lang_d %>% 
  filter(is_new) %>% 
  group_by(named_run_type) %>% 
  summarize(mean_loss=mean(loss))
```



# continual learning 
Note: the code for running these experiments is in the continual_learning branch, but the analysis code is here to facilitate generating unified figures for the paper.

```{r}
continual_results_dir = "../results/continual_learning/"
continual_result_subdirs = c("hyper_emb_only_even_faster",  "hyper_emb_only_even_faster_randomly_initialized_baseline",  "hyper_emb_only_even_faster_untrained_baseline") 
```

```{r}
continual_d = load_d(continual_results_dir, continual_result_subdirs, num_runs, "new_losses") %>% 
  select(epoch, run, named_run_type, run_type,
         matches("\\d\\.\\d\\d"), -matches("add|permute|mult|square")) %>% # all base tasks have at least one coefficient
  gather(polynomial, loss, -epoch, -contains("run")) %>% 
  mutate(run_type = case_when(run_type == "hyper" ~ "hyper_continual", 
                              run_type == "conditioned" ~ "conditioned_continual",
                              run_type == "hyper_emb_only_even_faster" ~ "hyper_continual_frozen",
                              run_type == "hyper_emb_only_even_faster_untrained_baseline" ~ "hyper_continual_frozen_untrained_baseline",
                              run_type == "hyper_emb_only_even_faster_randomly_initialized_baseline" ~ "hyper_continual_frozen_randomly_initialized_baseline",
                              run_type == "conditioned_emb_only_even_faster" ~ "conditioned_continual_frozen",
                              run_type == "conditioned_emb_only_even_faster_untrained_baseline" ~ "conditioned_continual_frozen_untrained_baseline",
                              T ~ run_type),
         named_run_type = name_run_type(run_type))
```


```{r}
continual_config_d = load_d(continual_results_dir, continual_result_subdirs, num_runs, "config") %>% 
  mutate(run_type = case_when(run_type == "hyper" ~ "hyper_continual", 
                              run_type == "conditioned" ~ "conditioned_continual",
                              run_type == "hyper_emb_only_even_faster" ~ "hyper_continual_frozen",
                              run_type == "hyper_emb_only_even_faster_untrained_baseline" ~ "hyper_continual_frozen_untrained_baseline",
                              run_type == "hyper_emb_only_even_faster_randomly_initialized_baseline" ~ "hyper_continual_frozen_randomly_initialized_baseline",
                              run_type == "conditioned_emb_only_even_faster" ~ "conditioned_continual_frozen",
                              run_type == "conditioned_emb_only_even_faster_untrained_baseline" ~ "conditioned_continual_frozen_untrained_baseline",
                              T ~ run_type),
         named_run_type = name_run_type(run_type))

```

```{r}
config_d = bind_rows(config_d, continual_config_d)
```

```{r}
continual_d = continual_d %>%
  rowwise() %>%
  mutate(is_new = check_if_new_task(run, run_type, polynomial)) %>%
  ungroup() %>%
  mutate(learned="After training",
         is_new = factor(is_new, labels=c("Prior tasks", "New tasks")),
         named_run_type = factor(named_run_type, levels=c("Tuning new task embeddings", "... from random embeddings", "... in an untrained network")))
```
  
```{r}
summary_plot(continual_d %>%
               filter(!grepl("conditioned", named_run_type),
                      epoch <= 1000,
                      run_type != "hyper_continual_frozen_randomly_initialized_baseline" | epoch > 0), # the first eval step of random init is actually with guess embeddings
             "epoch", "loss", "is_new",
             continuous_x = T) +
  labs(x = "Epoch", y="Loss") +
  facet_wrap(~ named_run_type)
ggsave("../../meta_RL/writing/figures/continual/continual_learning.png", width=8, height=3)
```

```{r}
summary_plot(continual_d %>%
               filter(!grepl("conditioned", named_run_type),
                      run_type != "hyper_continual_frozen_randomly_initialized_baseline" | epoch > 0,
                      is_new == "New tasks"), # the first eval step of random init is actually with guess embeddings
             "epoch", "loss", "named_run_type",
             continuous_x = T) +
  labs(x = "Epoch", y="Loss on new tasks")
ggsave("../../meta_RL/writing/figures/continual/continual_learning_direct_comparison.png", width=6, height=3)
```

```{r}
library(lme4)
```

```{r}
lmer(loss ~ named_run_type + (1|run),
   data=continual_d %>% 
     group_by(run_type, named_run_type) %>%
     filter(epoch == 300) %>%
     ungroup()) %>%
  summary()
```
